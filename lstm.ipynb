{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9MRbrxtJ+O1n8jiRtGwJh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Japmanpreet-ctrl/LSTM-NEXT-WORD-PREDICTOR/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezSOMnqXOO09"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"\n",
        "India is a country with a long and complex history that spans thousands of years. The culture of India has been shaped by traditions, languages, beliefs, and social practices that continue to evolve over time. People in India speak many languages, and language plays an important role in everyday communication, education, and identity. Hindi and English are widely used, but regional languages are equally important in different states and communities.\n",
        "\n",
        "Language influences how people think, learn, and express ideas. In Maharashtra, people commonly speak Marathi, and the language is closely connected to local culture, literature, and history. Marathi poetry and writing reflect social values and traditions passed down through generations. Similarly, in Punjab, Punjabi is spoken and represents the spirit, emotions, and lifestyle of the region. Each language carries stories, emotions, and knowledge that connect people to their roots.\n",
        "\n",
        "Education has always been valued in Indian society. Traditionally, learning was passed through teachers, books, and personal guidance. In modern times, education has expanded beyond classrooms and textbooks. Students now use digital tools, online platforms, and interactive resources to learn new skills. Technology has made education more accessible to people living in cities as well as rural areas.\n",
        "\n",
        "Technology is changing the way people work, learn, and communicate. Computers, mobile phones, and the internet have become essential parts of daily life. Access to information has increased rapidly, allowing people to learn at their own pace. With the growth of technology, fields such as data science, machine learning, and artificial intelligence have gained significant importance in recent years.\n",
        "\n",
        "Machine learning is a branch of artificial intelligence that focuses on learning patterns from data. Instead of writing fixed rules, developers train models using large amounts of data. These models improve performance by learning from experience. Deep learning is a part of machine learning that uses neural networks inspired by the structure of the human brain. These networks are capable of understanding complex relationships in data.\n",
        "\n",
        "Neural networks are used in many real world applications. They are applied in speech recognition, image classification, recommendation systems, and language processing. Language models can predict the next word in a sentence by learning from text data. Such models understand context and sequence, which makes them useful for tasks like text generation and auto completion.\n",
        "\n",
        "Learning new technologies requires patience, consistency, and practice. Beginners often start with programming languages such as Python because it is easy to read and widely used in industry. Programming helps people develop logical thinking and problem solving skills. Writing code regularly improves confidence and understanding over time.\n",
        "\n",
        "Practice plays a crucial role in learning. Repeating concepts, experimenting with projects, and making mistakes are all part of the learning process. Students who practice consistently tend to understand concepts more deeply. Learning improves gradually as experience increases, and challenges become opportunities for growth.\n",
        "\n",
        "The future of learning is closely connected to technology and innovation. As tools evolve, people will continue to adapt and learn in new ways. Education, language, and technology together shape how societies grow and progress. Understanding these connections helps individuals prepare for the challenges of the modern world and build meaningful solutions.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import  tensorflow as tf"
      ],
      "metadata": {
        "id": "B8qNMCo8PmbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "7A3uZMjLPqXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wEK1bfOwPyMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TqpK-6YP8by",
        "outputId": "77a85867-6a9b-41f1-a0b8-a19b42b2af67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.word_index"
      ],
      "metadata": {
        "id": "qvrK7q4DP-bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sentences = re.split(r'[.!?]\\s+', corpus)\n",
        "sequa=[]\n",
        "sequences = []\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    sequa.append(tokens)\n",
        "    for i in range(1, len(tokens)):\n",
        "        sequences.append(tokens[:i+1])\n",
        "print(sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZAswuTpQG_l",
        "outputId": "8759340d-ca23-4844-8487-124cb660aae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[23, 8], [23, 8, 10], [23, 8, 10, 81], [23, 8, 10, 81, 19], [23, 8, 10, 81, 19, 10], [23, 8, 10, 81, 19, 10, 82], [23, 8, 10, 81, 19, 10, 82, 1], [23, 8, 10, 81, 19, 10, 82, 1, 35], [23, 8, 10, 81, 19, 10, 82, 1, 35, 36], [23, 8, 10, 81, 19, 10, 82, 1, 35, 36, 13], [23, 8, 10, 81, 19, 10, 82, 1, 35, 36, 13, 83], [23, 8, 10, 81, 19, 10, 82, 1, 35, 36, 13, 83, 84], [23, 8, 10, 81, 19, 10, 82, 1, 35, 36, 13, 83, 84, 3], [23, 8, 10, 81, 19, 10, 82, 1, 35, 36, 13, 83, 84, 3, 37], [4, 38], [4, 38, 3], [4, 38, 3, 23], [4, 38, 3, 23, 14], [4, 38, 3, 23, 14, 39], [4, 38, 3, 23, 14, 39, 85], [4, 38, 3, 23, 14, 39, 85, 20], [4, 38, 3, 23, 14, 39, 85, 20, 40], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41, 87], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41, 87, 13], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41, 87, 13, 42], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41, 87, 13, 42, 6], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41, 87, 13, 42, 6, 43], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41, 87, 13, 42, 6, 43, 44], [4, 38, 3, 23, 14, 39, 85, 20, 40, 21, 86, 1, 41, 87, 13, 42, 6, 43, 44, 45], [7, 2], [7, 2, 23], [7, 2, 23, 46], [7, 2, 23, 46, 47], [7, 2, 23, 46, 47, 21], [7, 2, 23, 46, 47, 21, 1], [7, 2, 23, 46, 47, 21, 1, 9], [7, 2, 23, 46, 47, 21, 1, 9, 48], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49, 50], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49, 50, 2], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49, 50, 2, 89], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49, 50, 2, 89, 90], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49, 50, 2, 89, 90, 15], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49, 50, 2, 89, 90, 15, 1], [7, 2, 23, 46, 47, 21, 1, 9, 48, 88, 49, 50, 2, 89, 90, 15, 1, 91], [92, 1], [92, 1, 93], [92, 1, 93, 11], [92, 1, 93, 11, 51], [92, 1, 93, 11, 51, 24], [92, 1, 93, 11, 51, 24, 94], [92, 1, 93, 11, 51, 24, 94, 95], [92, 1, 93, 11, 51, 24, 94, 95, 21], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11, 96], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11, 96, 49], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11, 96, 49, 2], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11, 96, 49, 2, 97], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11, 96, 49, 2, 97, 98], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11, 96, 49, 2, 97, 98, 1], [92, 1, 93, 11, 51, 24, 94, 95, 21, 11, 96, 49, 2, 97, 98, 1, 99], [9, 100], [9, 100, 52], [9, 100, 52, 7], [9, 100, 52, 7, 101], [9, 100, 52, 7, 101, 16], [9, 100, 52, 7, 101, 16, 1], [9, 100, 52, 7, 101, 16, 1, 102], [9, 100, 52, 7, 101, 16, 1, 102, 103], [2, 104], [2, 104, 7], [2, 104, 7, 105], [2, 104, 7, 105, 46], [2, 104, 7, 105, 46, 53], [2, 104, 7, 105, 46, 53, 1], [2, 104, 7, 105, 46, 53, 1, 4], [2, 104, 7, 105, 46, 53, 1, 4, 9], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54, 55], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54, 55, 6], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54, 55, 6, 106], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54, 55, 6, 106, 38], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54, 55, 6, 106, 38, 107], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54, 55, 6, 106, 38, 107, 1], [2, 104, 7, 105, 46, 53, 1, 4, 9, 8, 54, 55, 6, 106, 38, 107, 1, 36], [53, 108], [53, 108, 1], [53, 108, 1, 25], [53, 108, 1, 25, 109], [53, 108, 1, 25, 109, 41], [53, 108, 1, 25, 109, 41, 110], [53, 108, 1, 25, 109, 41, 110, 1], [53, 108, 1, 25, 109, 41, 110, 1, 40], [53, 108, 1, 25, 109, 41, 110, 1, 40, 56], [53, 108, 1, 25, 109, 41, 110, 1, 40, 56, 111], [53, 108, 1, 25, 109, 41, 110, 1, 40, 56, 111, 57], [53, 108, 1, 25, 109, 41, 110, 1, 40, 56, 111, 57, 112], [113, 2], [113, 2, 114], [113, 2, 114, 115], [113, 2, 114, 115, 8], [113, 2, 114, 115, 8, 116], [113, 2, 114, 115, 8, 116, 1], [113, 2, 114, 115, 8, 116, 1, 117], [113, 2, 114, 115, 8, 116, 1, 117, 4], [113, 2, 114, 115, 8, 116, 1, 117, 4, 118], [113, 2, 114, 115, 8, 116, 1, 117, 4, 118, 58], [113, 2, 114, 115, 8, 116, 1, 117, 4, 118, 58, 1], [113, 2, 114, 115, 8, 116, 1, 117, 4, 118, 58, 1, 119], [113, 2, 114, 115, 8, 116, 1, 117, 4, 118, 58, 1, 119, 3], [113, 2, 114, 115, 8, 116, 1, 117, 4, 118, 58, 1, 119, 3, 4], [113, 2, 114, 115, 8, 116, 1, 117, 4, 118, 58, 1, 119, 3, 4, 120], [121, 9], [121, 9, 122], [121, 9, 122, 123], [121, 9, 122, 123, 58], [121, 9, 122, 123, 58, 1], [121, 9, 122, 123, 58, 1, 124], [121, 9, 122, 123, 58, 1, 124, 13], [121, 9, 122, 123, 58, 1, 124, 13, 125], [121, 9, 122, 123, 58, 1, 124, 13, 125, 7], [121, 9, 122, 123, 58, 1, 124, 13, 125, 7, 6], [121, 9, 122, 123, 58, 1, 124, 13, 125, 7, 6, 59], [121, 9, 122, 123, 58, 1, 124, 13, 125, 7, 6, 59, 126], [15, 14], [15, 14, 127], [15, 14, 127, 39], [15, 14, 127, 39, 128], [15, 14, 127, 39, 128, 2], [15, 14, 127, 39, 128, 2, 129], [15, 14, 127, 39, 128, 2, 129, 130], [131, 5], [131, 5, 132], [131, 5, 132, 56], [131, 5, 132, 56, 57], [131, 5, 132, 56, 57, 133], [131, 5, 132, 56, 57, 133, 134], [131, 5, 132, 56, 57, 133, 134, 1], [131, 5, 132, 56, 57, 133, 134, 1, 135], [131, 5, 132, 56, 57, 133, 134, 1, 135, 136], [2, 60], [2, 60, 137], [2, 60, 137, 15], [2, 60, 137, 15, 14], [2, 60, 137, 15, 14, 138], [2, 60, 137, 15, 14, 138, 139], [2, 60, 137, 15, 14, 138, 139, 140], [2, 60, 137, 15, 14, 138, 139, 140, 1], [2, 60, 137, 15, 14, 138, 139, 140, 1, 141], [61, 142], [61, 142, 143], [61, 142, 143, 144], [61, 142, 143, 144, 62], [61, 142, 143, 144, 62, 145], [61, 142, 143, 144, 62, 145, 146], [61, 142, 143, 144, 62, 145, 146, 1], [61, 142, 143, 144, 62, 145, 146, 1, 147], [61, 142, 143, 144, 62, 145, 146, 1, 147, 148], [61, 142, 143, 144, 62, 145, 146, 1, 147, 148, 6], [61, 142, 143, 144, 62, 145, 146, 1, 147, 148, 6, 16], [61, 142, 143, 144, 62, 145, 146, 1, 147, 148, 6, 16, 26], [61, 142, 143, 144, 62, 145, 146, 1, 147, 148, 6, 16, 26, 63], [17, 14], [17, 14, 149], [17, 14, 149, 15], [17, 14, 149, 15, 64], [17, 14, 149, 15, 64, 150], [17, 14, 149, 15, 64, 150, 6], [17, 14, 149, 15, 64, 150, 6, 7], [17, 14, 149, 15, 64, 150, 6, 7, 151], [17, 14, 149, 15, 64, 150, 6, 7, 151, 2], [17, 14, 149, 15, 64, 150, 6, 7, 151, 2, 152], [17, 14, 149, 15, 64, 150, 6, 7, 151, 2, 152, 12], [17, 14, 149, 15, 64, 150, 6, 7, 151, 2, 152, 12, 153], [17, 14, 149, 15, 64, 150, 6, 7, 151, 2, 152, 12, 153, 12], [17, 14, 149, 15, 64, 150, 6, 7, 151, 2, 152, 12, 153, 12, 154], [17, 14, 149, 15, 64, 150, 6, 7, 151, 2, 152, 12, 153, 12, 154, 155], [17, 8], [17, 8, 156], [17, 8, 156, 4], [17, 8, 156, 4, 157], [17, 8, 156, 4, 157, 7], [17, 8, 156, 4, 157, 7, 158], [17, 8, 156, 4, 157, 7, 158, 16], [17, 8, 156, 4, 157, 7, 158, 16, 1], [17, 8, 156, 4, 157, 7, 158, 16, 1, 159], [160, 161], [160, 161, 162], [160, 161, 162, 1], [160, 161, 162, 1, 4], [160, 161, 162, 1, 4, 163], [160, 161, 162, 1, 4, 163, 65], [160, 161, 162, 1, 4, 163, 65, 66], [160, 161, 162, 1, 4, 163, 65, 66, 164], [160, 161, 162, 1, 4, 163, 65, 66, 164, 165], [160, 161, 162, 1, 4, 163, 65, 66, 164, 165, 3], [160, 161, 162, 1, 4, 163, 65, 66, 164, 165, 3, 166], [160, 161, 162, 1, 4, 163, 65, 66, 164, 165, 3, 166, 167], [168, 6], [168, 6, 169], [168, 6, 169, 14], [168, 6, 169, 14, 170], [168, 6, 169, 14, 170, 171], [168, 6, 169, 14, 170, 171, 172], [168, 6, 169, 14, 170, 171, 172, 7], [168, 6, 169, 14, 170, 171, 172, 7, 6], [168, 6, 169, 14, 170, 171, 172, 7, 6, 16], [168, 6, 169, 14, 170, 171, 172, 7, 6, 16, 173], [168, 6, 169, 14, 170, 171, 172, 7, 6, 16, 173, 59], [168, 6, 169, 14, 170, 171, 172, 7, 6, 16, 173, 59, 174], [168, 6, 169, 14, 170, 171, 172, 7, 6, 16, 173, 59, 174, 175], [19, 4], [19, 4, 67], [19, 4, 67, 3], [19, 4, 67, 3, 17], [19, 4, 67, 3, 17, 176], [19, 4, 67, 3, 17, 176, 27], [19, 4, 67, 3, 17, 176, 27, 12], [19, 4, 67, 3, 17, 176, 27, 12, 18], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69, 65], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69, 65, 178], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69, 65, 178, 179], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69, 65, 178, 179, 180], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69, 65, 178, 179, 180, 2], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69, 65, 178, 179, 180, 2, 181], [19, 4, 67, 3, 17, 176, 27, 12, 18, 177, 28, 5, 1, 68, 69, 65, 178, 179, 180, 2, 181, 37], [28, 5], [28, 5, 8], [28, 5, 8, 10], [28, 5, 8, 10, 182], [28, 5, 8, 10, 182, 3], [28, 5, 8, 10, 182, 3, 68], [28, 5, 8, 10, 182, 3, 68, 69], [28, 5, 8, 10, 182, 3, 68, 69, 13], [28, 5, 8, 10, 182, 3, 68, 69, 13, 183], [28, 5, 8, 10, 182, 3, 68, 69, 13, 183, 184], [28, 5, 8, 10, 182, 3, 68, 69, 13, 183, 184, 5], [28, 5, 8, 10, 182, 3, 68, 69, 13, 183, 184, 5, 185], [28, 5, 8, 10, 182, 3, 68, 69, 13, 183, 184, 5, 185, 29], [28, 5, 8, 10, 182, 3, 68, 69, 13, 183, 184, 5, 185, 29, 18], [186, 3], [186, 3, 25], [186, 3, 25, 187], [186, 3, 25, 187, 188], [186, 3, 25, 187, 188, 189], [186, 3, 25, 187, 188, 189, 190], [186, 3, 25, 187, 188, 189, 190, 22], [186, 3, 25, 187, 188, 189, 190, 22, 191], [186, 3, 25, 187, 188, 189, 190, 22, 191, 192], [186, 3, 25, 187, 188, 189, 190, 22, 191, 192, 193], [186, 3, 25, 187, 188, 189, 190, 22, 191, 192, 193, 3], [186, 3, 25, 187, 188, 189, 190, 22, 191, 192, 193, 3, 18], [30, 22], [30, 22, 194], [30, 22, 194, 195], [30, 22, 194, 195, 20], [30, 22, 194, 195, 20, 5], [30, 22, 194, 195, 20, 5, 29], [30, 22, 194, 195, 20, 5, 29, 70], [196, 5], [196, 5, 8], [196, 5, 8, 10], [196, 5, 8, 10, 71], [196, 5, 8, 10, 71, 3], [196, 5, 8, 10, 71, 3, 28], [196, 5, 8, 10, 71, 3, 28, 5], [196, 5, 8, 10, 71, 3, 28, 5, 13], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198, 20], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198, 20, 4], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198, 20, 4, 199], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198, 20, 4, 199, 3], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198, 20, 4, 199, 3, 4], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198, 20, 4, 199, 3, 4, 200], [196, 5, 8, 10, 71, 3, 28, 5, 13, 197, 72, 31, 198, 20, 4, 199, 3, 4, 200, 201], [30, 31], [30, 31, 11], [30, 31, 11, 202], [30, 31, 11, 202, 3], [30, 31, 11, 202, 3, 32], [30, 31, 11, 202, 3, 32, 35], [30, 31, 11, 202, 3, 32, 35, 203], [30, 31, 11, 202, 3, 32, 35, 203, 2], [30, 31, 11, 202, 3, 32, 35, 203, 2, 18], [72, 31], [72, 31, 11], [72, 31, 11, 24], [72, 31, 11, 24, 2], [72, 31, 11, 24, 2, 47], [72, 31, 11, 24, 2, 47, 204], [72, 31, 11, 24, 2, 47, 204, 73], [72, 31, 11, 24, 2, 47, 204, 73, 205], [206, 11], [206, 11, 207], [206, 11, 207, 2], [206, 11, 207, 2, 208], [206, 11, 207, 2, 208, 209], [206, 11, 207, 2, 208, 209, 210], [206, 11, 207, 2, 208, 209, 210, 211], [206, 11, 207, 2, 208, 209, 210, 211, 212], [206, 11, 207, 2, 208, 209, 210, 211, 212, 213], [206, 11, 207, 2, 208, 209, 210, 211, 212, 213, 1], [206, 11, 207, 2, 208, 209, 210, 211, 212, 213, 1, 9], [206, 11, 207, 2, 208, 209, 210, 211, 212, 213, 1, 9, 214], [9, 22], [9, 22, 215], [9, 22, 215, 216], [9, 22, 215, 216, 4], [9, 22, 215, 216, 4, 217], [9, 22, 215, 216, 4, 217, 218], [9, 22, 215, 216, 4, 217, 218, 2], [9, 22, 215, 216, 4, 217, 218, 2, 10], [9, 22, 215, 216, 4, 217, 218, 2, 10, 219], [9, 22, 215, 216, 4, 217, 218, 2, 10, 219, 20], [9, 22, 215, 216, 4, 217, 218, 2, 10, 219, 20, 5], [9, 22, 215, 216, 4, 217, 218, 2, 10, 219, 20, 5, 29], [9, 22, 215, 216, 4, 217, 218, 2, 10, 219, 20, 5, 29, 74], [9, 22, 215, 216, 4, 217, 218, 2, 10, 219, 20, 5, 29, 74, 18], [27, 22], [27, 22, 75], [27, 22, 75, 220], [27, 22, 75, 220, 1], [27, 22, 75, 220, 1, 221], [27, 22, 75, 220, 1, 221, 222], [27, 22, 75, 220, 1, 221, 222, 223], [27, 22, 75, 220, 1, 221, 222, 223, 224], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33, 226], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33, 226, 227], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33, 226, 227, 74], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33, 226, 227, 74, 228], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33, 226, 227, 74, 228, 1], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33, 226, 227, 74, 228, 1, 229], [27, 22, 75, 220, 1, 221, 222, 223, 224, 225, 33, 226, 227, 74, 228, 1, 229, 230], [5, 26], [5, 26, 231], [5, 26, 231, 232], [5, 26, 231, 232, 233], [5, 26, 231, 232, 233, 234], [5, 26, 231, 232, 233, 234, 1], [5, 26, 231, 232, 233, 234, 1, 34], [235, 236], [235, 236, 237], [235, 236, 237, 19], [235, 236, 237, 19, 76], [235, 236, 237, 19, 76, 21], [235, 236, 237, 19, 76, 21, 27], [235, 236, 237, 19, 76, 21, 27, 12], [235, 236, 237, 19, 76, 21, 27, 12, 238], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241, 6], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241, 6, 242], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241, 6, 242, 1], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241, 6, 242, 1, 51], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241, 6, 242, 1, 51, 24], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241, 6, 242, 1, 51, 24, 2], [235, 236, 237, 19, 76, 21, 27, 12, 238, 239, 240, 8, 241, 6, 242, 1, 51, 24, 2, 243], [76, 77], [76, 77, 7], [76, 77, 7, 244], [76, 77, 7, 244, 245], [76, 77, 7, 244, 245, 246], [76, 77, 7, 244, 245, 246, 1], [76, 77, 7, 244, 245, 246, 1, 247], [76, 77, 7, 244, 245, 246, 1, 247, 248], [76, 77, 7, 244, 245, 246, 1, 247, 248, 63], [25, 249], [25, 249, 250], [25, 249, 250, 78], [25, 249, 250, 78, 251], [25, 249, 250, 78, 251, 1], [25, 249, 250, 78, 251, 1, 32], [25, 249, 250, 78, 251, 1, 32, 44], [25, 249, 250, 78, 251, 1, 32, 44, 45], [34, 48], [34, 48, 10], [34, 48, 10, 252], [34, 48, 10, 252, 50], [34, 48, 10, 252, 50, 2], [34, 48, 10, 252, 50, 2, 5], [253, 79], [253, 79, 254], [253, 79, 254, 19], [253, 79, 254, 19, 255], [253, 79, 254, 19, 255, 1], [253, 79, 254, 19, 255, 1, 256], [253, 79, 254, 19, 255, 1, 256, 257], [253, 79, 254, 19, 255, 1, 256, 257, 11], [253, 79, 254, 19, 255, 1, 256, 257, 11, 258], [253, 79, 254, 19, 255, 1, 256, 257, 11, 258, 71], [253, 79, 254, 19, 255, 1, 256, 257, 11, 258, 71, 3], [253, 79, 254, 19, 255, 1, 256, 257, 11, 258, 71, 3, 4], [253, 79, 254, 19, 255, 1, 256, 257, 11, 258, 71, 3, 4, 5], [253, 79, 254, 19, 255, 1, 256, 257, 11, 258, 71, 3, 4, 5, 259], [61, 260], [61, 260, 34], [61, 260, 34, 261], [61, 260, 34, 261, 262], [61, 260, 34, 261, 262, 6], [61, 260, 34, 261, 262, 6, 75], [61, 260, 34, 261, 262, 6, 75, 79], [61, 260, 34, 261, 262, 6, 75, 79, 64], [61, 260, 34, 261, 262, 6, 75, 79, 64, 263], [5, 78], [5, 78, 264], [5, 78, 264, 12], [5, 78, 264, 12, 70], [5, 78, 264, 12, 70, 265], [5, 78, 264, 12, 70, 265, 1], [5, 78, 264, 12, 70, 265, 1, 80], [5, 78, 264, 12, 70, 265, 1, 80, 66], [5, 78, 264, 12, 70, 265, 1, 80, 66, 266], [5, 78, 264, 12, 70, 265, 1, 80, 66, 266, 33], [5, 78, 264, 12, 70, 265, 1, 80, 66, 266, 33, 67], [4, 267], [4, 267, 3], [4, 267, 3, 5], [4, 267, 3, 5, 8], [4, 267, 3, 5, 8, 54], [4, 267, 3, 5, 8, 54, 55], [4, 267, 3, 5, 8, 54, 55, 6], [4, 267, 3, 5, 8, 54, 55, 6, 17], [4, 267, 3, 5, 8, 54, 55, 6, 17, 1], [4, 267, 3, 5, 8, 54, 55, 6, 17, 1, 268], [12, 62], [12, 62, 43], [12, 62, 43, 7], [12, 62, 43, 7, 269], [12, 62, 43, 7, 269, 42], [12, 62, 43, 7, 269, 42, 6], [12, 62, 43, 7, 269, 42, 6, 270], [12, 62, 43, 7, 269, 42, 6, 270, 1], [12, 62, 43, 7, 269, 42, 6, 270, 1, 16], [12, 62, 43, 7, 269, 42, 6, 270, 1, 16, 2], [12, 62, 43, 7, 269, 42, 6, 270, 1, 16, 2, 26], [12, 62, 43, 7, 269, 42, 6, 270, 1, 16, 2, 26, 271], [15, 9], [15, 9, 1], [15, 9, 1, 17], [15, 9, 1, 17, 272], [15, 9, 1, 17, 272, 273], [15, 9, 1, 17, 272, 273, 52], [15, 9, 1, 17, 272, 273, 52, 274], [15, 9, 1, 17, 272, 273, 52, 274, 275], [15, 9, 1, 17, 272, 273, 52, 274, 275, 1], [15, 9, 1, 17, 272, 273, 52, 274, 275, 1, 276], [32, 30], [32, 30, 277], [32, 30, 277, 77], [32, 30, 277, 77, 278], [32, 30, 277, 77, 278, 279], [32, 30, 277, 77, 278, 279, 33], [32, 30, 277, 77, 278, 279, 33, 4], [32, 30, 277, 77, 278, 279, 33, 4, 80], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3, 4], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3, 4, 60], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3, 4, 60, 73], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3, 4, 60, 73, 1], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3, 4, 60, 73, 1, 280], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3, 4, 60, 73, 1, 280, 281], [32, 30, 277, 77, 278, 279, 33, 4, 80, 3, 4, 60, 73, 1, 280, 281, 282]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=max(len(x) for x in sequences)"
      ],
      "metadata": {
        "id": "CsiU0oVUQc2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGICEqaASZNA",
        "outputId": "addd836d-a655-48f7-e974-a57ba4cb8aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "481"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sequences = pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=max_len,\n",
        "    padding=\"pre\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "660plvaUSt7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KWjp1zoS5Ho",
        "outputId": "c83619cb-fd55-4b35-f091-e0f9d71dcf89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  23,   8],\n",
              "       [  0,   0,   0, ...,  23,   8,  10],\n",
              "       [  0,   0,   0, ...,   8,  10,  81],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  73,   1, 280],\n",
              "       [  0,   0,   0, ...,   1, 280, 281],\n",
              "       [  0,   0,   0, ..., 280, 281, 282]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = sequences[:, :-1]   # all words except last\n",
        "y = sequences[:, -1]    # last word (target)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNW2EmCZTHUB",
        "outputId": "c8f4eba2-3b8f-46f7-d223-a851f53087fb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (481, 21)\n",
            "y shape: (481,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf3CFk_OaYR8",
        "outputId": "cd71cb93-604d-46bb-ca0d-069b844fdbde"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=100,\n",
        "        input_length=max_len-1\n",
        "    ),\n",
        "    LSTM(150),\n",
        "    Dropout(0.2),\n",
        "    Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "PA030_LqTaok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "11e99a51-f8c4-4075-cd57-5f39f3185ff4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=150,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B6tCtuBaWLQ",
        "outputId": "2c52e051-02ea-485a-90c2-536f419b5a2a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.0303 - loss: 5.6436\n",
            "Epoch 2/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.0638 - loss: 5.5852\n",
            "Epoch 3/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0780 - loss: 5.3089\n",
            "Epoch 4/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0678 - loss: 5.2590\n",
            "Epoch 5/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0618 - loss: 5.2183\n",
            "Epoch 6/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0724 - loss: 5.1801\n",
            "Epoch 7/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0717 - loss: 5.1316\n",
            "Epoch 8/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.0589 - loss: 5.1573\n",
            "Epoch 9/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.0751 - loss: 5.1207\n",
            "Epoch 10/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0627 - loss: 5.0834\n",
            "Epoch 11/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0739 - loss: 4.9439\n",
            "Epoch 12/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0600 - loss: 4.9847\n",
            "Epoch 13/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.0696 - loss: 4.8808\n",
            "Epoch 14/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0800 - loss: 4.8469\n",
            "Epoch 15/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0969 - loss: 4.8162\n",
            "Epoch 16/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0688 - loss: 4.7122\n",
            "Epoch 17/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0645 - loss: 4.8001\n",
            "Epoch 18/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0950 - loss: 4.6349\n",
            "Epoch 19/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0902 - loss: 4.5792\n",
            "Epoch 20/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0827 - loss: 4.6010\n",
            "Epoch 21/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0833 - loss: 4.4955\n",
            "Epoch 22/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0826 - loss: 4.4174\n",
            "Epoch 23/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1121 - loss: 4.2974\n",
            "Epoch 24/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1143 - loss: 4.2582\n",
            "Epoch 25/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1304 - loss: 4.1507\n",
            "Epoch 26/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1164 - loss: 4.0828\n",
            "Epoch 27/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1249 - loss: 4.0316\n",
            "Epoch 28/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.1530 - loss: 3.8741\n",
            "Epoch 29/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.1561 - loss: 3.7517\n",
            "Epoch 30/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.1897 - loss: 3.6806\n",
            "Epoch 31/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2190 - loss: 3.5551\n",
            "Epoch 32/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.2041 - loss: 3.5033\n",
            "Epoch 33/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2770 - loss: 3.3656\n",
            "Epoch 34/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2502 - loss: 3.2931\n",
            "Epoch 35/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2582 - loss: 3.2205\n",
            "Epoch 36/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3076 - loss: 3.0519\n",
            "Epoch 37/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3527 - loss: 2.9752\n",
            "Epoch 38/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3695 - loss: 2.9016\n",
            "Epoch 39/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4077 - loss: 2.7576\n",
            "Epoch 40/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4023 - loss: 2.7904\n",
            "Epoch 41/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4666 - loss: 2.6358\n",
            "Epoch 42/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4641 - loss: 2.5177\n",
            "Epoch 43/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5026 - loss: 2.4489\n",
            "Epoch 44/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4939 - loss: 2.4315\n",
            "Epoch 45/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5327 - loss: 2.2696\n",
            "Epoch 46/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.6069 - loss: 2.1910\n",
            "Epoch 47/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6159 - loss: 2.1076\n",
            "Epoch 48/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5961 - loss: 2.0686\n",
            "Epoch 49/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6206 - loss: 2.0150\n",
            "Epoch 50/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.6444 - loss: 1.8947\n",
            "Epoch 51/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6861 - loss: 1.8246\n",
            "Epoch 52/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6754 - loss: 1.7780\n",
            "Epoch 53/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7282 - loss: 1.6754\n",
            "Epoch 54/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7739 - loss: 1.6280\n",
            "Epoch 55/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7475 - loss: 1.6417\n",
            "Epoch 56/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7583 - loss: 1.5538\n",
            "Epoch 57/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7747 - loss: 1.5243\n",
            "Epoch 58/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7905 - loss: 1.4510\n",
            "Epoch 59/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8061 - loss: 1.4135\n",
            "Epoch 60/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8123 - loss: 1.3295\n",
            "Epoch 61/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8504 - loss: 1.3171\n",
            "Epoch 62/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8550 - loss: 1.2774\n",
            "Epoch 63/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8700 - loss: 1.1588\n",
            "Epoch 64/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8648 - loss: 1.1420\n",
            "Epoch 65/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8847 - loss: 1.1333\n",
            "Epoch 66/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8891 - loss: 1.0620\n",
            "Epoch 67/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9169 - loss: 1.0259\n",
            "Epoch 68/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9056 - loss: 1.0064\n",
            "Epoch 69/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9069 - loss: 0.9415\n",
            "Epoch 70/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9243 - loss: 0.9320\n",
            "Epoch 71/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9455 - loss: 0.8669\n",
            "Epoch 72/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.9216 - loss: 0.8945\n",
            "Epoch 73/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9197 - loss: 0.8594\n",
            "Epoch 74/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9225 - loss: 0.8082\n",
            "Epoch 75/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9696 - loss: 0.7671\n",
            "Epoch 76/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9377 - loss: 0.7759\n",
            "Epoch 77/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9519 - loss: 0.7287\n",
            "Epoch 78/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9597 - loss: 0.7183\n",
            "Epoch 79/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9479 - loss: 0.7002\n",
            "Epoch 80/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9547 - loss: 0.6752\n",
            "Epoch 81/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9654 - loss: 0.6493\n",
            "Epoch 82/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9631 - loss: 0.6318\n",
            "Epoch 83/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9629 - loss: 0.5996\n",
            "Epoch 84/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9811 - loss: 0.5789\n",
            "Epoch 85/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9555 - loss: 0.5590\n",
            "Epoch 86/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9707 - loss: 0.5447\n",
            "Epoch 87/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9666 - loss: 0.5056\n",
            "Epoch 88/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9626 - loss: 0.5001\n",
            "Epoch 89/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9693 - loss: 0.4949\n",
            "Epoch 90/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9751 - loss: 0.4807\n",
            "Epoch 91/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9817 - loss: 0.4572\n",
            "Epoch 92/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9786 - loss: 0.4376\n",
            "Epoch 93/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9767 - loss: 0.4625\n",
            "Epoch 94/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9688 - loss: 0.4322\n",
            "Epoch 95/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9752 - loss: 0.4157\n",
            "Epoch 96/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9841 - loss: 0.3915\n",
            "Epoch 97/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9804 - loss: 0.3891\n",
            "Epoch 98/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9832 - loss: 0.3612\n",
            "Epoch 99/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9799 - loss: 0.3681\n",
            "Epoch 100/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9765 - loss: 0.3719\n",
            "Epoch 101/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9814 - loss: 0.3511\n",
            "Epoch 102/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9787 - loss: 0.3336\n",
            "Epoch 103/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9708 - loss: 0.3362\n",
            "Epoch 104/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9890 - loss: 0.3353\n",
            "Epoch 105/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9746 - loss: 0.3192\n",
            "Epoch 106/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9812 - loss: 0.3155\n",
            "Epoch 107/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9810 - loss: 0.3029\n",
            "Epoch 108/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9812 - loss: 0.3130\n",
            "Epoch 109/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9842 - loss: 0.2815\n",
            "Epoch 110/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9867 - loss: 0.2796\n",
            "Epoch 111/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9861 - loss: 0.2785\n",
            "Epoch 112/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9875 - loss: 0.2668\n",
            "Epoch 113/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9860 - loss: 0.2498\n",
            "Epoch 114/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9710 - loss: 0.2725\n",
            "Epoch 115/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9862 - loss: 0.2560\n",
            "Epoch 116/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9834 - loss: 0.2351\n",
            "Epoch 117/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9795 - loss: 0.2447\n",
            "Epoch 118/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9825 - loss: 0.2092\n",
            "Epoch 119/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9667 - loss: 0.2486\n",
            "Epoch 120/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9840 - loss: 0.2219\n",
            "Epoch 121/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9773 - loss: 0.2354\n",
            "Epoch 122/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9835 - loss: 0.2027\n",
            "Epoch 123/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9843 - loss: 0.2023\n",
            "Epoch 124/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9759 - loss: 0.2065\n",
            "Epoch 125/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9792 - loss: 0.2109\n",
            "Epoch 126/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9810 - loss: 0.1974\n",
            "Epoch 127/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9808 - loss: 0.2073\n",
            "Epoch 128/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9773 - loss: 0.1947\n",
            "Epoch 129/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9909 - loss: 0.1741\n",
            "Epoch 130/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9739 - loss: 0.1900\n",
            "Epoch 131/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9851 - loss: 0.1775\n",
            "Epoch 132/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9810 - loss: 0.1736\n",
            "Epoch 133/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9760 - loss: 0.1735\n",
            "Epoch 134/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9862 - loss: 0.1676\n",
            "Epoch 135/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9830 - loss: 0.1629\n",
            "Epoch 136/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9731 - loss: 0.1703\n",
            "Epoch 137/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9886 - loss: 0.1668\n",
            "Epoch 138/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9886 - loss: 0.1611\n",
            "Epoch 139/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9907 - loss: 0.1561\n",
            "Epoch 140/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9742 - loss: 0.1591\n",
            "Epoch 141/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9750 - loss: 0.1508\n",
            "Epoch 142/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9781 - loss: 0.1591\n",
            "Epoch 143/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9893 - loss: 0.1476\n",
            "Epoch 144/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9808 - loss: 0.1388\n",
            "Epoch 145/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9946 - loss: 0.1368\n",
            "Epoch 146/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9889 - loss: 0.1404\n",
            "Epoch 147/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9901 - loss: 0.1317\n",
            "Epoch 148/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9880 - loss: 0.1278\n",
            "Epoch 149/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9780 - loss: 0.1291\n",
            "Epoch 150/150\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9915 - loss: 0.1280\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e3893598890>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def predict_next_words(seed_text, n_words=5):\n",
        "    for _ in range(n_words):\n",
        "        seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        seq = pad_sequences([seq], maxlen=max_len-1, padding=\"pre\")\n",
        "        preds = model.predict(seq, verbose=0)\n",
        "        next_idx = np.argmax(preds)\n",
        "\n",
        "        for word, idx in tokenizer.word_index.items():\n",
        "            if idx == next_idx:\n",
        "                seed_text += \" \" + word\n",
        "                break\n",
        "    return seed_text\n"
      ],
      "metadata": {
        "id": "jP-pxpb4b2gE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "2vYG1rJ-ci3U"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_predict(seed_text, num_words):\n",
        "    return predict_next_words(seed_text, int(num_words))\n"
      ],
      "metadata": {
        "id": "8ZqTncPQct_Z"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.Interface(\n",
        "    fn=gradio_predict,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Enter seed text\",\n",
        "            placeholder=\"e.g. machine learning\"\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=1,\n",
        "            maximum=20,\n",
        "            step=1,\n",
        "            value=5,\n",
        "            label=\"Number of words to predict\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated text\"),\n",
        "    title=\"Next Word Prediction using LSTM\",\n",
        "    description=\"This model predicts the next words based on learned language patterns using an LSTM network.\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "5csU-qIHcvp1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "RBSK850-cyZH",
        "outputId": "164478bf-0a11-478a-a2e9-bc05dc1fe2c9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f2ba8ec2019e685efe.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f2ba8ec2019e685efe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQmLgSeGc0Wz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}